{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f059292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def predict_stock_direction(ticker, n_days, n_hist_wk, fe=None, n_components=10, n_jobs=24):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # ================== Data Preparation ====================\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(weeks=n_hist_wk)\n",
    "\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "\n",
    "    data['price_change'] = data['Close'].pct_change()\n",
    "    data['volume_change'] = data['Volume'].pct_change()\n",
    "    data['relative_high'] = (data['High'] - data['Close']) / data['Close']\n",
    "    data['relative_low'] = (data['Low'] - data['Close']) / data['Close']\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    # Add recent n_days data\n",
    "    for i in range(n_days, 0, -1):\n",
    "        feature_df[f'price_change_{i}'] = data['price_change'].shift(i)\n",
    "        feature_df[f'volume_change_{i}'] = data['volume_change'].shift(i)\n",
    "        feature_df[f'relative_high_{i}'] = data['relative_high'].shift(i)\n",
    "        feature_df[f'relative_low_{i}'] = data['relative_low'].shift(i)\n",
    "    \n",
    "    # Add other factor\n",
    "    feature_df['day_in_week'] = data.index.dayofweek\n",
    "    feature_df['day_in_month'] = data.index.day\n",
    "    feature_df['month_in_year'] = data.index.month\n",
    "    feature_df['quarter'] = data.index.quarter\n",
    "\n",
    "    feature_df['direction'] = (data['Close'].pct_change() > 0).astype(int)\n",
    "    feature_df = feature_df.dropna()\n",
    "    \n",
    "    total_up_days = feature_df['direction'].sum()\n",
    "    total_days = feature_df.shape[0]\n",
    "    baseline_accuracy_ratio = total_up_days / total_days\n",
    "    print('... Data prepared! Data size:', feature_df.shape)\n",
    "    print(f\"Baseline accuracy ratio: {baseline_accuracy_ratio:.2f}\")\n",
    "\n",
    "    # =================== Train-test split =========================\n",
    "    X = feature_df.drop(columns=['direction'])\n",
    "    y = feature_df['direction']\n",
    "\n",
    "    X_train_ori, X_test_ori, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print('... Train/Tests splitted! Train size:', X_train_ori.shape)\n",
    "    \n",
    "    # Normalize the input data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_normalized = scaler.fit_transform(X_train_ori)\n",
    "    X_test_normalized = scaler.transform(X_test_ori)\n",
    "\n",
    "    # PCA model\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    input_dim = X_train_normalized.shape[1]\n",
    "    encoding_dim = n_components\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoder_layer = Dense(input_dim, activation='linear')(encoder_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    X_train = None\n",
    "    X_test  = None\n",
    "    if fe == 'pca':\n",
    "        X_train = pca.fit_transform(X_train_normalized)\n",
    "        X_test = pca.transform(X_test_normalized)\n",
    "        print('... PCA done! Train size:', X_train.shape)\n",
    "    elif fe == 'ae':\n",
    "        autoencoder.fit(X_train_normalized, X_train_normalized,\n",
    "                    epochs=100,  # You can choose the number of epochs\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test_normalized, X_test_normalized))\n",
    "        encoder = Model(inputs=input_layer, outputs=encoder_layer)\n",
    "        X_train = encoder.predict(X_train_normalized)\n",
    "        X_test = encoder.predict(X_test_normalized)\n",
    "        print('... Autoencoder trained! Train size:', X_train.shape)\n",
    "    else:\n",
    "        print('... No dimensionality reduction will be performed. Train size:', X_train.shape)\n",
    "    \n",
    "    scoring_metrics = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'f1_score': make_scorer(f1_score)\n",
    "    }\n",
    "    \n",
    "    print('... Model training started!')\n",
    "    \n",
    "    # ==================== Logistic Regression =======================\n",
    "    log_reg = LogisticRegression(random_state=42)\n",
    "    param_grid5 = {\n",
    "        'penalty': ['l2', 'none'],\n",
    "        'C': np.logspace(-4, 4, 9),\n",
    "        'fit_intercept': [True, False],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': [100, 500, 1000, 2000]\n",
    "    }\n",
    "    grid_search_log_reg = GridSearchCV(\n",
    "        log_reg, param_grid5, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    grid_search_log_reg.fit(X_train, y_train)\n",
    "    #print(f'... Best Logistic Regression accuracy on training set: {grid_search_log_reg.best_score_:.4f}')\n",
    "    best_log_reg = grid_search_log_reg.best_estimator_\n",
    "    y_pred = best_log_reg.predict(X_test)\n",
    "    print(f'... Best Logistic Regression accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # =================== Support vector machine =========================\n",
    "    #svm = SVC()\n",
    "    #param_grid1 = {\n",
    "    #    'C': np.logspace(-2, 2, 5),\n",
    "    #    'kernel': ['linear'],\n",
    "    #}\n",
    "    #grid_search_svm = GridSearchCV(\n",
    "    #    svm, param_grid1, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    #grid_search_svm.fit(X_train, y_train)\n",
    "    #print(f'... Best SVM accuracy on training set: {grid_search_svm.best_score_:.4f}')\n",
    "    #best_svm = grid_search_svm.best_estimator_\n",
    "    #y_pred = best_svm.predict(X_test)\n",
    "    #print(f'... Best SVM accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # ==================== Random forest =======================\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    param_grid2 = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt'],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        rf, param_grid2, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    #print(f'... Best Random forest accuracy on training set: {grid_search_rf.best_score_:.4f}')\n",
    "    best_rf = grid_search_rf.best_estimator_\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    print(f'... Best Random forest accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # ==================== Gradient boost =======================\n",
    "    gbm = GradientBoostingClassifier(random_state=42)\n",
    "    param_grid3 = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 8],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt']\n",
    "    }\n",
    "    grid_search_gbm = GridSearchCV(\n",
    "        gbm, param_grid3, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    grid_search_gbm.fit(X_train, y_train)\n",
    "    #print(f'... Best Gradient boost accuracy on training set: {grid_search_gbm.best_score_:.4f}')\n",
    "    best_gbm = grid_search_gbm.best_estimator_\n",
    "    y_pred = best_gbm.predict(X_test)\n",
    "    print(f'... Best Gradient boost accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "    \n",
    "    # ==================== XGBoost Classifier =======================\n",
    "    xgb_clf = xgb.XGBClassifier(random_state=42, objective='binary:logistic')\n",
    "    param_grid6 = {\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5],\n",
    "        'min_child_weight': [1, 3],\n",
    "        'gamma': [0, 0.1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "    }\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        xgb_clf, param_grid6, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "    #print(f'... Best XGBoost accuracy on training set: {grid_search_xgb.best_score_:.4f}')\n",
    "    best_xgb = grid_search_xgb.best_estimator_\n",
    "    y_pred = best_xgb.predict(X_test)\n",
    "    print(f'... Best XGBoost accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "    \n",
    "    # ==================== Multi-Layer Percepton =======================\n",
    "    mlp = MLPClassifier(random_state=42)\n",
    "    param_grid4 = {\n",
    "        'hidden_layer_sizes': [(50, 50), (100, 100), (50, 100), (100, 50), (50, 50, 50), \n",
    "                               (100, 50, 100), (50, 100, 50), (50, 100, 100),\n",
    "                               (100, 100, 100), (50, 50, 50, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "    }\n",
    "    grid_search_mlp = GridSearchCV(\n",
    "        mlp, param_grid4, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    grid_search_mlp.fit(X_train, y_train)\n",
    "    #print(f'... Best MLP accuracy on training set: {grid_search_mlp.best_score_:.4f}')\n",
    "    best_mlp = grid_search_mlp.best_estimator_\n",
    "    y_pred = best_mlp.predict(X_test)\n",
    "    print(f'... Best MLP accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # ==================== k-Nearest Neighbors (k-NN) Classifier =======================\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    param_grid_knn = {\n",
    "        'n_neighbors': list(range(1, 31)),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "    grid_search_knn = GridSearchCV(\n",
    "        knn, param_grid_knn, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    grid_search_knn.fit(X_train, y_train)\n",
    "    #print(f'... Best k-NN accuracy on training set: {grid_search_knn.best_score_:.4f}')\n",
    "    best_knn = grid_search_knn.best_estimator_\n",
    "    y_pred = best_knn.predict(X_test)\n",
    "    print(f'... Best k-NN accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # ==================== Naive Bayes Classifier =======================\n",
    "    from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "    # Choose the appropriate Naive Bayes Classifier based on your data type\n",
    "    naive_bayes = GaussianNB()\n",
    "    # naive_bayes = MultinomialNB()\n",
    "    # naive_bayes = BernoulliNB()\n",
    "\n",
    "    naive_bayes.fit(X_train, y_train)\n",
    "    #print(f'... Naive Bayes accuracy on training set: {cross_val_score(naive_bayes, X_train, y_train, cv=5, scoring=\"accuracy\").mean():.4f}')\n",
    "    y_pred = naive_bayes.predict(X_test)\n",
    "    print(f'... Naive Bayes accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # ==================== Decision Trees =======================\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    param_grid_dt = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': [None, 'sqrt', 'log2']\n",
    "    }\n",
    "    grid_search_dt = GridSearchCV(\n",
    "        dt, param_grid_dt, scoring=scoring_metrics, refit='accuracy', cv=5, n_jobs=n_jobs)\n",
    "    grid_search_dt.fit(X_train, y_train)\n",
    "    #print(f'... Best Decision tree accuracy on training set: {grid_search_dt.best_score_:.4f}')\n",
    "    best_dt = grid_search_dt.best_estimator_\n",
    "    y_pred = best_dt.predict(X_test)\n",
    "    print(f'... Best Decision tree accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # ==================== Stacking =======================\n",
    "    stacked_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            #('svm', best_svm),\n",
    "            ('random_forest', best_rf),\n",
    "            ('gbm', best_gbm),\n",
    "            ('mlp', best_mlp),\n",
    "            ('logreg', best_log_reg),\n",
    "            ('xgboost', best_xgb),\n",
    "            ('knn', best_knn),\n",
    "            ('DT', best_dt),\n",
    "            ('naive_bayes', naive_bayes)\n",
    "         ],\n",
    "        final_estimator=LogisticRegression(random_state=42),\n",
    "        cv=5,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    stacked_clf.fit(X_train, y_train)\n",
    "    y_pred = stacked_clf.predict(X_test)\n",
    "    print(f'... Stacking accuracy on test set: {accuracy_score(y_test, y_pred):.4f}')\n",
    "\n",
    "    # ==================== Prediction =======================\n",
    "    last_n_days_data = X.iloc[-1:]\n",
    "    last_n_days_data_normalized = scaler.transform(last_n_days_data)\n",
    "\n",
    "    if fe == 'pca':\n",
    "        last_n_days_data_transformed = pca.transform(last_n_days_data_normalized)\n",
    "        feature_columns = [f'PC{i + 1}' for i in range(n_components)]\n",
    "    elif fe == 'ae':\n",
    "        last_n_days_data_transformed = encoder.predict(last_n_days_data_normalized)\n",
    "        feature_columns = [f'AE{i + 1}' for i in range(encoding_dim)]\n",
    "    else:\n",
    "        last_n_days_data_transformed = last_n_days_data_normalized\n",
    "        feature_columns = X.columns\n",
    "\n",
    "    last_n_days_data_with_feature_names = pd.DataFrame(\n",
    "        data=last_n_days_data_transformed,\n",
    "        columns=feature_columns\n",
    "    )\n",
    "\n",
    "    # Make predictions using the best models\n",
    "    #tomorrow_direction_svm = best_svm.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_rf = best_rf.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_gbm = best_gbm.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_mlp = best_mlp.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_logreg = best_log_reg.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_xgb = best_xgb.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_knn = best_knn.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_dt = best_dt.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_naive_bayes = naive_bayes.predict(last_n_days_data_with_feature_names)\n",
    "    tomorrow_direction_stacked = stacked_clf.predict(last_n_days_data_with_feature_names)\n",
    "\n",
    "    # Print the predictions for tomorrow\n",
    "    models = {\n",
    "        \"Logistic Regression\": tomorrow_direction_logreg[0],\n",
    "        #\"SVM\": tomorrow_direction_svm[0],\n",
    "        \"Random Forest\": tomorrow_direction_rf[0],\n",
    "        \"GBM\": tomorrow_direction_gbm[0],\n",
    "        \"MLP\": tomorrow_direction_mlp[0],\n",
    "        \"XGBoost\": tomorrow_direction_xgb[0],\n",
    "        \"KNN\": tomorrow_direction_knn[0],\n",
    "        \"DesTree\": tomorrow_direction_dt[0],\n",
    "        \"NaiveBayes\": tomorrow_direction_naive_bayes[0],\n",
    "        \"Stacking\": tomorrow_direction_stacked[0],\n",
    "    }\n",
    "\n",
    "    print(f\"Predictions for {ticker} tomorrow:\")\n",
    "    for model_name, direction in models.items():\n",
    "        print(f\"{model_name}: {'UP' if direction == 1 else 'DOWN'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57545e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "... Data prepared! Data size: (3852, 25)\n",
      "Baseline accuracy ratio: 0.55\n",
      "... Train/Tests splitted! Train size: (3081, 24)\n",
      "... PCA done! Train size: (3081, 15)\n",
      "... Model training started!\n",
      "... Best Logistic Regression accuracy on test set: 0.5499\n",
      "... Best Random forest accuracy on test set: 0.5512\n",
      "... Best Gradient boost accuracy on test set: 0.5447\n",
      "... Best XGBoost accuracy on test set: 0.5460\n",
      "... Best MLP accuracy on test set: 0.5058\n",
      "... Best k-NN accuracy on test set: 0.5058\n",
      "... Naive Bayes accuracy on test set: 0.5551\n",
      "... Best Decision tree accuracy on test set: 0.5240\n",
      "... Stacking accuracy on test set: 0.5447\n",
      "Predictions for QQQ tomorrow:\n",
      "Logistic Regression: UP\n",
      "Random Forest: DOWN\n",
      "GBM: UP\n",
      "MLP: DOWN\n",
      "XGBoost: UP\n",
      "KNN: DOWN\n",
      "DesTree: UP\n",
      "NaiveBayes: UP\n",
      "Stacking: UP\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "... Data prepared! Data size: (3837, 85)\n",
      "Baseline accuracy ratio: 0.55\n",
      "... Train/Tests splitted! Train size: (3069, 84)\n",
      "... PCA done! Train size: (3069, 20)\n",
      "... Model training started!\n",
      "... Best Logistic Regression accuracy on test set: 0.5508\n",
      "... Best Random forest accuracy on test set: 0.5391\n",
      "... Best Gradient boost accuracy on test set: 0.5547\n",
      "... Best XGBoost accuracy on test set: 0.5430\n",
      "... Best MLP accuracy on test set: 0.5130\n",
      "... Best k-NN accuracy on test set: 0.5260\n",
      "... Naive Bayes accuracy on test set: 0.5286\n",
      "... Best Decision tree accuracy on test set: 0.5247\n",
      "... Stacking accuracy on test set: 0.5443\n",
      "Predictions for QQQ tomorrow:\n",
      "Logistic Regression: UP\n",
      "Random Forest: UP\n",
      "GBM: UP\n",
      "MLP: DOWN\n",
      "XGBoost: UP\n",
      "KNN: DOWN\n",
      "DesTree: DOWN\n",
      "NaiveBayes: UP\n",
      "Stacking: UP\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "... Data prepared! Data size: (3797, 245)\n",
      "Baseline accuracy ratio: 0.55\n",
      "... Train/Tests splitted! Train size: (3037, 244)\n",
      "... PCA done! Train size: (3037, 30)\n",
      "... Model training started!\n",
      "... Best Logistic Regression accuracy on test set: 0.5526\n",
      "... Best Random forest accuracy on test set: 0.5421\n",
      "... Best Gradient boost accuracy on test set: 0.5500\n",
      "... Best XGBoost accuracy on test set: 0.5526\n",
      "... Best MLP accuracy on test set: 0.4987\n",
      "... Best k-NN accuracy on test set: 0.5316\n",
      "... Naive Bayes accuracy on test set: 0.5250\n",
      "... Best Decision tree accuracy on test set: 0.5368\n",
      "... Stacking accuracy on test set: 0.5395\n",
      "Predictions for QQQ tomorrow:\n",
      "Logistic Regression: UP\n",
      "Random Forest: UP\n",
      "GBM: UP\n",
      "MLP: DOWN\n",
      "XGBoost: UP\n",
      "KNN: UP\n",
      "DesTree: UP\n",
      "NaiveBayes: UP\n",
      "Stacking: UP\n"
     ]
    }
   ],
   "source": [
    "predict_stock_direction('QQQ', 5, 800, 'pca', 15)\n",
    "predict_stock_direction('QQQ', 20, 800, 'pca', 20)\n",
    "predict_stock_direction('QQQ', 60, 800, 'pca', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87014434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "... Data prepared! Data size: (3852, 25)\n",
      "Baseline accuracy ratio: 0.54\n",
      "... Train/Tests splitted! Train size: (3081, 24)\n",
      "... PCA done! Train size: (3081, 15)\n",
      "... Model training started!\n",
      "... Best Logistic Regression accuracy on test set: 0.5577\n",
      "... Best Random forest accuracy on test set: 0.5305\n",
      "... Best Gradient boost accuracy on test set: 0.5396\n",
      "... Best XGBoost accuracy on test set: 0.5240\n",
      "... Best MLP accuracy on test set: 0.5110\n",
      "... Best k-NN accuracy on test set: 0.5175\n",
      "... Naive Bayes accuracy on test set: 0.5435\n",
      "... Best Decision tree accuracy on test set: 0.5149\n",
      "... Stacking accuracy on test set: 0.5422\n",
      "Predictions for SPY tomorrow:\n",
      "Logistic Regression: UP\n",
      "Random Forest: DOWN\n",
      "GBM: UP\n",
      "MLP: DOWN\n",
      "XGBoost: UP\n",
      "KNN: DOWN\n",
      "DesTree: UP\n",
      "NaiveBayes: UP\n",
      "Stacking: UP\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "... Data prepared! Data size: (3837, 85)\n",
      "Baseline accuracy ratio: 0.54\n",
      "... Train/Tests splitted! Train size: (3069, 84)\n",
      "... PCA done! Train size: (3069, 20)\n",
      "... Model training started!\n",
      "... Best Logistic Regression accuracy on test set: 0.5404\n",
      "... Best Random forest accuracy on test set: 0.5312\n",
      "... Best Gradient boost accuracy on test set: 0.5417\n",
      "... Best XGBoost accuracy on test set: 0.5195\n",
      "... Best MLP accuracy on test set: 0.4831\n",
      "... Best k-NN accuracy on test set: 0.4844\n",
      "... Naive Bayes accuracy on test set: 0.5195\n",
      "... Best Decision tree accuracy on test set: 0.5052\n",
      "... Stacking accuracy on test set: 0.5456\n",
      "Predictions for SPY tomorrow:\n",
      "Logistic Regression: UP\n",
      "Random Forest: UP\n",
      "GBM: UP\n",
      "MLP: DOWN\n",
      "XGBoost: UP\n",
      "KNN: DOWN\n",
      "DesTree: UP\n",
      "NaiveBayes: UP\n",
      "Stacking: UP\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "... Data prepared! Data size: (3797, 245)\n",
      "Baseline accuracy ratio: 0.54\n",
      "... Train/Tests splitted! Train size: (3037, 244)\n",
      "... PCA done! Train size: (3037, 30)\n",
      "... Model training started!\n",
      "... Best Logistic Regression accuracy on test set: 0.5368\n",
      "... Best Random forest accuracy on test set: 0.5500\n",
      "... Best Gradient boost accuracy on test set: 0.5513\n",
      "... Best XGBoost accuracy on test set: 0.5395\n",
      "... Best MLP accuracy on test set: 0.5000\n",
      "... Best k-NN accuracy on test set: 0.4882\n",
      "... Naive Bayes accuracy on test set: 0.5263\n",
      "... Best Decision tree accuracy on test set: 0.4882\n",
      "... Stacking accuracy on test set: 0.5500\n",
      "Predictions for SPY tomorrow:\n",
      "Logistic Regression: UP\n",
      "Random Forest: DOWN\n",
      "GBM: DOWN\n",
      "MLP: DOWN\n",
      "XGBoost: UP\n",
      "KNN: UP\n",
      "DesTree: DOWN\n",
      "NaiveBayes: UP\n",
      "Stacking: DOWN\n"
     ]
    }
   ],
   "source": [
    "predict_stock_direction('SPY', 5, 800, 'pca', 15)\n",
    "predict_stock_direction('SPY', 20, 800, 'pca', 20)\n",
    "predict_stock_direction('SPY', 60, 800, 'pca', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc870526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
